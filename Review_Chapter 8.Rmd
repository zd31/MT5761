---
title: "Review_Chapter 8"
author: "Ziteng"
output: html_document
---

# GLMs for binary data

In chapter 5, we modeled counts which is ***Poisson*** distributed data, and the response is ***Expected Counts ($\lambda_{it}$)***; in chapter 6&7, we modeled probabilities/proportions for Binomial distributed data, and the response is ***probability/proportion ($p$ and $p_{it}$)***. 

In this chapter, we are still going to model the probability of sighting birds. But different with considering data as Binomial distributions with 25 $p_i$s and $n_i$s by pooling them by year-month, we will consider each observation as a Bernoulli or one-trial Binomial.

## 8.4 Model fitting

We can fit Binomial/Bernoulli-based GLMs for proportional/binary data using logit/probit/complementary log-log link using Maximum Likelihood.

## 8.7 Model selection

Anova

Information criteria: AIC/BIC/AICc

## 8.8 Predictive power

Since we are fitting binary data, it is less useful of plotting the observed values against fitted values, because the responses are binary, but fitted values are probabilities.

**An R-squared metric**

Only for binary data:
$$
R^2 = \frac{1-e^{\frac{D-D_{null}}{N}}}{1-e^{-\frac{D_{null}}{N}}}
$$
where $N$ is the total number of binary observations; $D$, deviance, is a measure of fit between fitted model and saturated model; $D_{null}$ is the deviance of the ***intercept only*** model.

If $D=D_{null}$, it means the covariates are not working at all.

***Low in $R^2$ does not mean a poor fit.***

We can use the fitted value as the response variable and re-fit the model, to see if there is a large difference in $R^2$ between the fitted and re-fitted models. 
We would expect a small difference if the fit is good.

## 8.9 The confusion matrix

This is a way of assessing model performance for binary data.

|             |   Observed 0   |   Observed 1   |
|:-----------:|:--------------:|:--------------:|
|Predicted 0  |  True Negative |  False Negative|
|Predicted 1  |  False Positive|  True Positive |

Since the fitted values are probabilities but the responses are binary, we need a ***threshold value*** to determine one fitted value is 1 or 0. If the fitted value is larger than the threshold value, then it is 1; if the fitted value is smaller than the threshold value, then it is 0.
A common choice is the mean of the fitted values.

How many are correctly classified?
$$
\frac{True~Negative + True~Positive}{Total~Number~of~Observations}
$$
How many are incorrectly predicted 'failures'?
$$
\frac{False~Negative}{True~Negative + False~Negative} = \frac{False~Negative}{Predicted~0}
$$
How many are incorrectly predicted 'successes'?
$$
\frac{False~Positive}{True~Positive + False~Positive}
$$

**Receiver Operating Characteristic (ROC) Curves**

It can be use to assess the predictive power at ALL threshold values.

***Sensitivity***: the proportion of correctly classified 1s.
$$
\frac{True~Positive}{True~Positive + False~Positive}=\frac{True~Positive}{Observed~1}
$$
It is the probability that we predict seeing birds when we actually observed birds.

***Specificity***: the proportion of correctly classified 0s.
$$
\frac{True~Negative}{True~Negative + False~Negative}=\frac{True~Negative}{Observed~0}
$$
It is the probability that we predict not seeing birds when we actually observed no birds.

The ***best*** threshold is the one maximizes the true positives and minimizes the false positives.


## 8.10 Model diagnostics

### 8.10.1 Linearity on the link scale

Partial residual plots are useless for binary data.

Partial plots in `effects` might be useful.

### 8.10.2 Assessing the mean-variance relationship

There is no dispersion for binary data as there is only one trial in each case, but it is necessary when the number trails are larger than 1.

For Binomial data, we would expect pattern in plot of fitted values against raw residuals, but no pattern in plot of fitted value against Pearson residuals.

### 8.10.3 Non-independence in model residuals

ACF



## Big Note

Fitting Binomial GLMs and Bernoulli GLMs would give the same results if all else are the same.









