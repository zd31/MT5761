---
title: "Chapter 2"
author: "Ziteng"
output: html_document
---

```{r include=FALSE}
# Load necessary packages
library(tidyverse)
```

```{r}
# Read in data
df <- read.csv("NystedFarms.csv")

head(df)
```



We can write the model in this way
$$
y_{it} = linear \ predictor + error \ term_{it} \\
E(y_{it}) = \mu_{it} = linear \ predictor
$$
t denotes the occasion (not phase), i denotes the location.
The error term is assumed to be normally distributed with mean == 0 and variance == sigma^2.

### ?There is no error in second expression, and each y_it is i.i.d and normally distributed with mean == mu_it? what is the variance and the relation with distribution of residual

The variables we choose includes continuous and categorical variables. 
And the intercept of our model means when all continuous variables == 0 at baseline.
The coefficients of continuous variables tell us (only in this model), the expected change in density per unit in variable.
The coefficients of categories variables tell us, the expected change in density relative to the baseline.

Above we call main effects.
### Now we consider interactions, which give us additional information about the slope of specific continuous variable??? what is the combination of interactions?


# 2.2 Model fitting
What do we mean by fitting a model?
We want the "distance" between sample data and fitted model to be the smallest. >> Least Squared
How we evaluate a statistic/estimate? <Consistent, Unbiased, Efficient> 
<B.L.U.E> Best Least Unbiased Estimate

Consistent means the more data you get, the smaller the variance from the true value.
Unbiased means the expected value is the true value.
Efficient means the variance are the smallest compared to other estimates.


R default the model has an intercept.

The df of covariates is the same with df of residual.

In the coefficient part of the outcome of regression, each row is test separately. Cannot remove insignificant variable according to this.


The residual standard error in regression output is the variance of residual.

Lower R-square does not mean the model is not a good one.

The F test in multiple regression, the null hypothesis is all coefficients are the same and equal to 0. Testing if the model is better than just a straight line.

avo & anova can do type I.
Anova in package(car) can do type II and III.


# 2.4 


# 2.5 Model selection


# 2.6 Check model assumption

Linearity
Explanatory variables have linear relation with response variable.
Method: partial residual plots >>> Covariates against residual

Normality --less important
The residual
Method: Q-Q plot

Constant variance --critical
Method: B-P test with null hypo that variance is constant.

Independence
Method: Durbin-Watson

Outliers and influential points
Method: cook's distance








